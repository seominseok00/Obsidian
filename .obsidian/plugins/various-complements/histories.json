{"models":{"models":{"currentFile":{"count":5,"lastUpdated":1755520160467}}},"reinforcement":{"reinforcement":{"currentFile":{"count":2,"lastUpdated":1755495929417}}},"learning":{"learning":{"currentFile":{"count":4,"lastUpdated":1755765430039}}},"uncertainty":{"uncertainty":{"currentFile":{"count":26,"lastUpdated":1755776099218}}},"Epistemic":{"Epistemic":{"currentFile":{"count":3,"lastUpdated":1755826545223}}},"epistemic":{"epistemic":{"currentFile":{"count":9,"lastUpdated":1755826724392}}},"performance":{"performance":{"currentFile":{"count":2,"lastUpdated":1755080669435}}},"parametric":{"parametric":{"currentFile":{"count":2,"lastUpdated":1755493005439}}},"approximators":{"approximators":{"currentFile":{"count":2,"lastUpdated":1755081717454}}},"asympotic":{"asympotic":{"currentFile":{"count":1,"lastUpdated":1755080667251}}},"methods":{"methods":{"currentFile":{"count":2,"lastUpdated":1755742964998}}},"aleatoric":{"aleatoric":{"currentFile":{"count":4,"lastUpdated":1755826381874}}},"distribution":{"distribution":{"currentFile":{"count":1,"lastUpdated":1755083535493}}},"probabilistic":{"probabilistic":{"currentFile":{"count":8,"lastUpdated":1755787101614}}},"discriminatively":{"discriminatively":{"currentFile":{"count":1,"lastUpdated":1755084766790}}},"networks":{"networks":{"currentFile":{"count":3,"lastUpdated":1756191833379}}},"corresponds":{"corresponds":{"currentFile":{"count":1,"lastUpdated":1755085597860}}},"Ensembles":{"Ensembles":{"currentFile":{"count":1,"lastUpdated":1755085619243}}},"inference":{"inference":{"currentFile":{"count":1,"lastUpdated":1755085676232}}},"However":{"However":{"currentFile":{"count":1,"lastUpdated":1755085694182}}},"bootstrapped":{"bootstrapped":{"currentFile":{"count":1,"lastUpdated":1755085909715}}},"Aleatoric":{"Aleatoric":{"currentFile":{"count":2,"lastUpdated":1755826393625}}},"improvement":{"improvement":{"currentFile":{"count":1,"lastUpdated":1755175556131}}},"\\begin{aligned}":{"\\begin{aligned}":{"currentFile":{"count":3,"lastUpdated":1756206823892}}},"\\end{aligned}":{"\\end{aligned}":{"currentFile":{"count":6,"lastUpdated":1756206829561}}},"Gradient":{"Gradient":{"currentFile":{"count":1,"lastUpdated":1755327652476}}},"TD-errorÍ∞Ä":{"TD-errorÍ∞Ä":{"currentFile":{"count":3,"lastUpdated":1755327998924}}},"dynamics":{"dynamics":{"currentFile":{"count":3,"lastUpdated":1755506214720}}},"experiments":{"experiments":{"currentFile":{"count":3,"lastUpdated":1755496339895}}},"Probabilistic":{"Probabilistic":{"currentFile":{"count":2,"lastUpdated":1755773405908}}},"multimodality":{"multimodality":{"currentFile":{"count":3,"lastUpdated":1755495170386}}},"separation":{"separation":{"currentFile":{"count":1,"lastUpdated":1755494688436}}},"nonparametric":{"nonparametric":{"currentFile":{"count":1,"lastUpdated":1755495952697}}},"asymptotic":{"asymptotic":{"currentFile":{"count":1,"lastUpdated":1755495985917}}},"efficient":{"efficient":{"currentFile":{"count":1,"lastUpdated":1755496000536}}},"incorporating":{"incorporating":{"currentFile":{"count":1,"lastUpdated":1755496174004}}},"environment":{"environment":{"currentFile":{"count":3,"lastUpdated":1755764076944}}},"objective":{"objective":{"currentFile":{"count":1,"lastUpdated":1755505984653}}},"unknown":{"unknown":{"currentFile":{"count":1,"lastUpdated":1755506220591}}},"Review":{"Review":{"currentFile":{"count":1,"lastUpdated":1755512608530}}},"dynamicsÏùò":{"dynamicsÏùò":{"currentFile":{"count":1,"lastUpdated":1755513425993}}},"‚ùì Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models":{"‚ùì Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models":{"internalLink":{"count":2,"lastUpdated":1755513429196}}},"indicator":{"indicator":{"currentFile":{"count":1,"lastUpdated":1755517025704}}},"Model-based":{"Model-based":{"currentFile":{"count":1,"lastUpdated":1755517058350}}},"Cross-entropy":{"Cross-entropy":{"currentFile":{"count":1,"lastUpdated":1755517250365}}},"Method":{"Method":{"currentFile":{"count":2,"lastUpdated":1756208156351}}},"imgs/resuilts":{"imgs/resuilts":{"internalLink":{"count":1,"lastUpdated":1755519789559}}},"Discussion":{"Discussion":{"currentFile":{"count":1,"lastUpdated":1755519981898}}},"Environment":{"Environment":{"currentFile":{"count":1,"lastUpdated":1755519995400}}},"‚ùì Neural network dynamics for model-based deep reinforcement learning with model-free Ô¨Åne-tuning":{"‚ùì Neural network dynamics for model-based deep reinforcement learning with model-free Ô¨Åne-tuning":{"internalLink":{"count":1,"lastUpdated":1755526563326}}},"Cross-Entropy":{"Cross-Entropy":{"currentFile":{"count":2,"lastUpdated":1755842539426}}},"Constrained":{"Constrained":{"currentFile":{"count":1,"lastUpdated":1755593293969}}},"constraint":{"constraint":{"currentFile":{"count":1,"lastUpdated":1755598104200}}},"Optimization":{"Optimization":{"currentFile":{"count":1,"lastUpdated":1755600856322}}},"{\\eo}":{"{\\eo}":{"currentFile":{"count":1,"lastUpdated":1755601204227}}},"{\\epsilon":{"{\\epsilon":{"currentFile":{"count":1,"lastUpdated":1755601206643}}},"\\epsilon":{"\\epsilon":{"currentFile":{"count":3,"lastUpdated":1755601274160}}},"‚ùì Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm":{"‚ùì Model-based Safe Deep Reinforcement Learning via a Constrained Proximal Policy Optimization Algorithm":{"internalLink":{"count":5,"lastUpdated":1756015957915}}},"\\tag{1}":{"\\tag{1}":{"currentFile":{"count":3,"lastUpdated":1755601826752}}},"algorithm":{"algorithm":{"currentFile":{"count":1,"lastUpdated":1755602106137}}},"Learning":{"Learning":{"currentFile":{"count":1,"lastUpdated":1755610034672}}},"Uncertainty":{"Uncertainty":{"currentFile":{"count":4,"lastUpdated":1755744500308}}},"Lagrangian":{"Lagrangian":{"currentFile":{"count":5,"lastUpdated":1756263264675}}},"Results":{"Results":{"currentFile":{"count":2,"lastUpdated":1756259311674}}},"learning.We":{"learning.We":{"currentFile":{"count":1,"lastUpdated":1755695834023}}},"approaches":{"approaches":{"currentFile":{"count":1,"lastUpdated":1755696147679}}},"details":{"details":{"currentFile":{"count":1,"lastUpdated":1755696380554}}},"hyperparameter":{"hyperparameter":{"currentFile":{"count":1,"lastUpdated":1755696389679}}},"Reinforcement":{"Reinforcement":{"currentFile":{"count":1,"lastUpdated":1755750639676}}},"safe-RL":{"safe-RL":{"currentFile":{"count":1,"lastUpdated":1755750645917}}},"planning":{"planning":{"currentFile":{"count":1,"lastUpdated":1755751265133}}},"Map-Elites":{"Map-Elites":{"currentFile":{"count":4,"lastUpdated":1755839145710}}},"algorithm1.png":{"algorithm1.png":{"currentFile":{"count":1,"lastUpdated":1755752901874}}},"behavior":{"behavior":{"currentFile":{"count":1,"lastUpdated":1755754180012}}},"functionÏùÑ":{"functionÏùÑ":{"currentFile":{"count":1,"lastUpdated":1755754456534}}},"rolloutÌïú":{"rolloutÌïú":{"currentFile":{"count":2,"lastUpdated":1755754594342}}},"‚ùì Guided Safe Shooting (model based reinforcement learning with safety constraints)":{"‚ùì Guided Safe Shooting (model based reinforcement learning with safety constraints)":{"internalLink":{"count":1,"lastUpdated":1755756204759}}},"auto-regressive":{"auto-regressive":{"currentFile":{"count":3,"lastUpdated":1755756327698}}},"DARMDN":{"DARMDN":{"currentFile":{"count":2,"lastUpdated":1755756429420}}},"\\text{SAMPLE}":{"\\text{SAMPLE}":{"currentFile":{"count":1,"lastUpdated":1755759716366}}},"{\\text{ME}}":{"{\\text{ME}}":{"currentFile":{"count":2,"lastUpdated":1755760386750}}},"safety-aware":{"safety-aware":{"currentFile":{"count":1,"lastUpdated":1755759925291}}},"generations":{"generations":{"currentFile":{"count":1,"lastUpdated":1755760010864}}},"\\text{SELECT}":{"\\text{SELECT}":{"currentFile":{"count":1,"lastUpdated":1755760039859}}},"SELECT":{"SELECT":{"currentFile":{"count":1,"lastUpdated":1755760170191}}},"trajectoriesÎ•º":{"trajectoriesÎ•º":{"currentFile":{"count":1,"lastUpdated":1755760366076}}},"Quality-Diversity":{"Quality-Diversity":{"currentFile":{"count":3,"lastUpdated":1755838840743}}},"evaluate":{"evaluate":{"currentFile":{"count":1,"lastUpdated":1755760768844}}},"Critique":{"Critique":{"currentFile":{"count":1,"lastUpdated":1755765280880}}},"#####":{"#####":{"currentFile":{"count":1,"lastUpdated":1755765673327}}},"Î∂àÌôïÏã§ÏÑ±ÏùÑ":{"Î∂àÌôïÏã§ÏÑ±ÏùÑ":{"currentFile":{"count":1,"lastUpdated":1755774430069}}},"Sergey-Levine":{"Sergey-Levine":{"currentFile":{"count":1,"lastUpdated":1755775514906}}},"uncertainty-aware":{"uncertainty-aware":{"currentFile":{"count":1,"lastUpdated":1755776100219}}},"samples":{"samples":{"currentFile":{"count":1,"lastUpdated":1755776134509}}},"üìö Neural network dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning":{"üìö Neural network dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning":{"internalLink":{"count":1,"lastUpdated":1755785123281}}},"uncertaintyÎ•º":{"uncertaintyÎ•º":{"currentFile":{"count":6,"lastUpdated":1755826707225}}},"horizonÏù¥Îûë":{"horizonÏù¥Îûë":{"currentFile":{"count":1,"lastUpdated":1755826957405}}},"Constrained Policy Optimization":{"üìö Constrained Policy Optimization":{"internalLink":{"count":1,"lastUpdated":1755837068399}}},"Auto-regressive":{"Auto-regressive":{"currentFile":{"count":3,"lastUpdated":1755838013403}}},"Networks":{"Networks":{"currentFile":{"count":2,"lastUpdated":1755837718902}}},"Evolution":{"Evolution":{"currentFile":{"count":1,"lastUpdated":1755838851853}}},"MAP-Elites":{"MAP-Elites":{"currentFile":{"count":1,"lastUpdated":1755839208664}}},"discretized":{"discretized":{"currentFile":{"count":1,"lastUpdated":1755839243669}}},"\\arg\\max":{"\\arg\\max":{"currentFile":{"count":1,"lastUpdated":1755841994464}}},"info.obs":{"info.obs":{"currentFile":{"count":1,"lastUpdated":1755930362610}}},"lidar":{"lidar":{"currentFile":{"count":1,"lastUpdated":1755933652836}}},"Lagrange Multiplier Network":{"Lagrange Multiplier Network":{"internalLink":{"count":2,"lastUpdated":1756013183052}}},"\\ref{}":{"\\ref{}":{"currentFile":{"count":2,"lastUpdated":1756114218862}}},"Population Based Training of Neural Networks":{"Population Based Training of Neural Networks":{"internalLink":{"count":1,"lastUpdated":1756190614490}}},"model":{"model":{"currentFile":{"count":1,"lastUpdated":1756190821434}}},"2007.03964":{"2007.03964":{"currentFile":{"count":1,"lastUpdated":1756202696102}}},"ÏóÖÎç∞Ïù¥Ìä∏Î•º":{"ÏóÖÎç∞Ïù¥Ìä∏Î•º":{"currentFile":{"count":1,"lastUpdated":1756202720937}}},"Multiplier":{"Multiplier":{"currentFile":{"count":1,"lastUpdated":1756208410309}}},"Proportional":{"Proportional":{"currentFile":{"count":1,"lastUpdated":1756208936401}}},"oscillation":{"oscillation":{"currentFile":{"count":6,"lastUpdated":1756263270047}}},"Purpose":{"Purpose":{"currentFile":{"count":1,"lastUpdated":1756210564044}}},"overshoot":{"overshoot":{"currentFile":{"count":2,"lastUpdated":1756263272518}}},"DoggoButton1":{"DoggoButton1":{"currentFile":{"count":1,"lastUpdated":1756261118967}}},"objectiveÏôÄ":{"objectiveÏôÄ":{"currentFile":{"count":1,"lastUpdated":1756262714732}}},"dynamical":{"dynamical":{"currentFile":{"count":1,"lastUpdated":1756263225341}}}}